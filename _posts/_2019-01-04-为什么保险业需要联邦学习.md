---
layout: post
comments: true
title: 为什么保险业需要联邦学习
published: true
---


联邦学习起源于2016年谷歌发表的一篇[文章](https://pmpml.github.io/PMPML16/papers/PMPML16_paper_20.pdf)和2017年的[博客](https://ai.googleblog.com/2017/04/federated-learning-collaborative.html)。其大意是谷歌要训练自家安卓系统的Gboard输入法模型，但又不希望把用户敏感的键盘数据上传到自家服务器（我在看着你呢搜狗同学）。所以与其让用户上传数据到云端服务器训练模型，谷歌选择让用户在自己的智能手机上单独训练一个模型（感谢各家芯片厂商的neural engine），然后把千万用户每个人的黑盒参数的模型上传去谷歌的云端服务器进行融合，更新官方模型，然后再推送还给用户。这样一方面避免了用户敏感数据的传输和存储，一方面又利用了用户智能手机的计算能力（即是edge computing）而大大减轻了集中服务器的计算压力。

联邦学习的概念在发表的时候，谷歌虽然提到了数据隐私，但论文核心重点是模型如何在手机上传带宽受限的情况下进行传输，而用户模型的融合也只是用了简单有效的参数平均。这么做的原因是因为工程上类似的想法在**分布式机器学习中**已经被探讨了很多，slave-master结构被广泛采用，参数服务器大放异彩，模型参数以致训练过程中的梯度都可以在同步或非同步的方式下计算和传递。联邦学习的关注重点于是放在了没有严格分布式计算环境、上传带宽受限、海量用户作为slave下的“工程实现”。

随着社会对数据隐私的日益关注和政府越来越多相关政策的出台，正在疯狂积累数据和逐渐创造价值的企业们忽然被束缚了手脚，急于寻找一个既能数据合规又能释放大数据潜力的方法。这个时候联邦学习对数据隐私的作用就立刻被人们认识到了。