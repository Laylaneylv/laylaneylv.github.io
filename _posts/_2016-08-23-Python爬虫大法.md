---
layout: post
comments: true
title: Python爬虫大法
published: true
---

这些天在学用Python大法写爬虫，为了绕坑尝试了几种不同技术。
大体来讲，很多以前python的爬虫包比如urllib,、urllib2（我也不懂为什么要有两个包）、正则表达式等等，都被更好用的requests、BeautifulSoup之类的代替了。
python爬虫大法，现在觉得最好用的就是requests + BeautifulSoup或者selenium + PhantomJS了，屠龙刀倚天剑各有优势。

爬虫有三个步骤。

* 第一步，打开冰箱：模拟人类的浏览器行为，遍历和找到目标网页。简单一点的网页，没有太多javascript或者asp.net之类的控件的话，就直接上requests大法，去post和get几下好了；再复杂一点的网页，对抓取速度又没有那么高要求的话，就用selenium以phantomjs做webdriver去一下一下点击好了，再不成功就去换个Firefox或者IE的webdriver。再仔细研究好网站遍历的方法，基本就能成功走到目标网页了。

* 第二步，把大象塞进冰箱：以前用正则，现在用BeautifulSoup去对目标网页做解析parse，找到需要的信息，存进一个格式清楚的数据结构里，比如json。

* 第三步，关上冰箱门：把方才的数据结构dump进硬盘里。

有时因为网站网页异构的原因，parse这步可以留在之后做，先去把网页源码全部存下来。这样妈妈再也不怕parse错误导致抓取中断或者还没parse完就被网站墙了之类的事情发生了。